{
 "bad": [
  "base_tokenizer.BaseTokenizer",
  "base_tokenizer.BaseTokenizer.__init__",
  "base_tokenizer.BaseTokenizer.__new__",
  "decoders.BPEDecoder",
  "decoders.ByteLevel",
  "decoders.Decoder",
  "decoders.Metaspace",
  "decoders.WordPiece",
  "models.BPE",
  "models.BPE.read_file",
  "models.Model",
  "models.Unigram",
  "models.WordLevel",
  "models.WordPiece",
  "normalizers.BertNormalizer",
  "normalizers.Lowercase",
  "normalizers.NFC",
  "normalizers.NFD",
  "normalizers.NFKC",
  "normalizers.NFKD",
  "normalizers.Nmt",
  "normalizers.Normalizer",
  "normalizers.Precompiled",
  "normalizers.Replace",
  "normalizers.Sequence",
  "normalizers.Strip",
  "normalizers.StripAccents",
  "normalizers.unicode_normalizer_from_str",
  "pre_tokenizers.BertPreTokenizer",
  "pre_tokenizers.ByteLevel",
  "pre_tokenizers.CharDelimiterSplit",
  "pre_tokenizers.Digits",
  "pre_tokenizers.Metaspace",
  "pre_tokenizers.PreTokenizer",
  "pre_tokenizers.Punctuation",
  "pre_tokenizers.Sequence",
  "pre_tokenizers.UnicodeScripts",
  "pre_tokenizers.Whitespace",
  "pre_tokenizers.WhitespaceSplit",
  "processors.BertProcessing",
  "processors.ByteLevel",
  "processors.PostProcessor",
  "processors.RobertaProcessing",
  "processors.TemplateProcessing",
  "sentencepiece_model_pb2.ModelProto",
  "sys.path.append",
  "tokenizers.AddedToken",
  "tokenizers.EncodeInput",
  "tokenizers.Encoding",
  "tokenizers.InputSequence",
  "tokenizers.PreTokenizedEncodeInput",
  "tokenizers.PreTokenizedInputSequence",
  "tokenizers.TextEncodeInput",
  "tokenizers.TextInputSequence",
  "tokenizers.Tokenizer",
  "tokenizers.decoders.ByteLevel",
  "tokenizers.decoders.Metaspace",
  "tokenizers.decoders.WordPiece",
  "tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer",
  "tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer",
  "tokenizers.implementations.char_level_bpe.AddedToken",
  "tokenizers.implementations.char_level_bpe.CharBPETokenizer",
  "tokenizers.implementations.char_level_bpe.Tokenizer",
  "tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer",
  "tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer",
  "tokenizers.models.BPE",
  "tokenizers.models.BPE.read_file",
  "tokenizers.models.Unigram",
  "tokenizers.models.WordPiece",
  "tokenizers.models.WordPiece.read_file",
  "tokenizers.normalizers.BertNormalizer",
  "tokenizers.normalizers.Lowercase",
  "tokenizers.normalizers.NFC",
  "tokenizers.normalizers.NFD",
  "tokenizers.normalizers.NFKC",
  "tokenizers.normalizers.NFKD",
  "tokenizers.normalizers.NORMALIZERS",
  "tokenizers.normalizers.Nmt",
  "tokenizers.normalizers.Normalizer",
  "tokenizers.normalizers.Precompiled",
  "tokenizers.normalizers.Sequence",
  "tokenizers.normalizers.unicode_normalizer_from_str",
  "tokenizers.pre_tokenizers.BertPreTokenizer",
  "tokenizers.pre_tokenizers.ByteLevel",
  "tokenizers.pre_tokenizers.ByteLevel.alphabet",
  "tokenizers.pre_tokenizers.Metaspace",
  "tokenizers.pre_tokenizers.Sequence",
  "tokenizers.pre_tokenizers.WhitespaceSplit",
  "tokenizers.processors.BertProcessing",
  "tokenizers.processors.ByteLevel",
  "tokenizers.trainers.BpeTrainer",
  "tokenizers.trainers.UnigramTrainer",
  "tokenizers.trainers.WordPieceTrainer",
  "trainers.BpeTrainer",
  "trainers.Trainer",
  "trainers.UnigramTrainer",
  "trainers.WordPieceTrainer",
  "typing.Dict",
  "typing.List",
  "typing.Optional",
  "typing.Tuple",
  "typing.Union"
 ],
 "deps": {
  "base_tokenizer": [],
  "decoders": [],
  "enum": [
   "privy/conda-forge/noarch/privy-6.0.0-py_0"
  ],
  "models": [],
  "normalizers": [],
  "pre_tokenizers": [],
  "processors": [],
  "sentencepiece_model_pb2": [],
  "sys": [],
  "tokenizers": [],
  "trainers": [],
  "typing": []
 }
}