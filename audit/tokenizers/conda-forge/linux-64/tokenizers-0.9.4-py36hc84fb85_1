{
 "bad": [
  "base_tokenizer.BaseTokenizer.__init__",
  "base_tokenizer.BaseTokenizer.__new__",
  "decoders.BPEDecoder",
  "decoders.ByteLevel",
  "decoders.Decoder",
  "decoders.Metaspace",
  "decoders.WordPiece",
  "models.BPE",
  "models.BPE.read_file",
  "models.Model",
  "models.Unigram",
  "models.WordLevel",
  "models.WordPiece",
  "normalizers.BertNormalizer",
  "normalizers.Lowercase",
  "normalizers.NFC",
  "normalizers.NFD",
  "normalizers.NFKC",
  "normalizers.NFKD",
  "normalizers.Nmt",
  "normalizers.Normalizer",
  "normalizers.Precompiled",
  "normalizers.Replace",
  "normalizers.Sequence",
  "normalizers.Strip",
  "normalizers.StripAccents",
  "pre_tokenizers.BertPreTokenizer",
  "pre_tokenizers.ByteLevel",
  "pre_tokenizers.CharDelimiterSplit",
  "pre_tokenizers.Digits",
  "pre_tokenizers.Metaspace",
  "pre_tokenizers.PreTokenizer",
  "pre_tokenizers.Punctuation",
  "pre_tokenizers.Sequence",
  "pre_tokenizers.UnicodeScripts",
  "pre_tokenizers.Whitespace",
  "pre_tokenizers.WhitespaceSplit",
  "processors.BertProcessing",
  "processors.ByteLevel",
  "processors.PostProcessor",
  "processors.RobertaProcessing",
  "processors.TemplateProcessing",
  "sentencepiece_model_pb2",
  "sentencepiece_model_pb2.ModelProto",
  "sys",
  "sys.path.append",
  "tokenizers.__init__.PreTokenizedEncodeInput",
  "tokenizers.__init__.PreTokenizedInputSequence",
  "tokenizers.__init__.TextEncodeInput",
  "tokenizers.__init__.TextInputSequence",
  "tokenizers.models.BPE.read_file",
  "tokenizers.models.WordPiece.read_file",
  "tokenizers.normalizers.__init__.NFC",
  "tokenizers.normalizers.__init__.NFD",
  "tokenizers.normalizers.__init__.NFKC",
  "tokenizers.normalizers.__init__.NFKD",
  "tokenizers.normalizers.__init__.NORMALIZERS",
  "tokenizers.normalizers.__init__.Normalizer",
  "tokenizers.pre_tokenizers.ByteLevel.alphabet",
  "tokenizers.tokenizers.AddedToken",
  "tokenizers.tokenizers.Encoding",
  "tokenizers.tokenizers.Tokenizer",
  "trainers.BpeTrainer",
  "trainers.Trainer",
  "trainers.UnigramTrainer",
  "trainers.WordPieceTrainer"
 ],
 "deps": {
  "base_tokenizer": [],
  "decoders": [],
  "enum": [
   "privy/conda-forge/noarch/privy-6.0.0-py_0",
   "python/conda-forge/linux-64/python-3.6.11-h4d41432_0_cpython"
  ],
  "json": [
   "python/conda-forge/linux-64/python-3.6.11-h4d41432_0_cpython"
  ],
  "models": [],
  "normalizers": [],
  "os": [
   "python/conda-forge/linux-64/python-3.6.11-h4d41432_0_cpython"
  ],
  "pre_tokenizers": [],
  "processors": [],
  "sentencepiece_model_pb2": [],
  "sys": [],
  "tokenizers": [],
  "trainers": [],
  "typing": [
   "python/conda-forge/linux-64/python-3.6.11-h4d41432_0_cpython"
  ]
 }
}